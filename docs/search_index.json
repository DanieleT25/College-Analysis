[["index.html", "College analysis Index Table of Contents", " College analysis Index Project’s name: College-Analysis Author: Daniele Tambone GitHub project: College-Analysis Date: 2025-08-28 Table of Contents Abstract 1. Introduction 2. Univariate Analysis 3. Dimensionality Reduction 4. Cluster Analysis 5. Conclusion "],["abstract.html", "Abstract", " Abstract This study analyzes the College dataset through univariate analysis, principal component analysis (PCA), and clustering methods. The univariate analysis highlighted that not all fitted models were appropriate for the individual variables. PCA revealed that the dataset is not optimally suited for dimensionality reduction, as the first two components alone explain less than 80% of the variance; however, applying Kaiser’s rule allowed the extraction of three interpretable principal components: Academic Prestige and Student Spending, Size and Enrollment Volume, and Personal and Book Expenses. Finally, clustering analysis showed that the k-means++ algorithm most effectively distinguished institutions, identifying two groups corresponding to private and public colleges, while the traditional k-means with three clusters uncovered additional patterns related to institutional size, selectivity, and student costs. "],["introduction.html", "1 Introduction", " 1 Introduction Understanding the landscape of higher education institutions in the United States is essential for students, policymakers, and educational organizations alike. Institutions vary significantly in terms of tuition costs, acceptance rates, graduation percentages, and student demographics. Uncovering the latent structure behind these variations can help identify common profiles among colleges and provide insights into the broader educational system. In this project, we apply unsupervised learning techniques to explore and analyze patterns in a dataset of U.S. colleges. Specifically, we leverage Principal Component Analysis (PCA) for dimensionality reduction and clustering algorithms to group similar institutions based on multiple quantitative features. The dataset used is the College dataset from the ISLP (Introduction to Statistical Learning with Python) package. It contains observations on 777 U.S. colleges, with variables covering aspects such as costs, enrollment, graduation rates, and student composition. Variable Descriptions The dataset includes a mix of numerical and categorical variables. Key attributes include: Private: whether the college is private or public. Apps: number of applications received. Accept: number of accepted students. Enroll: number of students enrolled. Top10perc: percentage of students from the top 10% of their high school class. Top25perc: percentage from the top 25%. F.Undergrad: number of full-time undergraduates. P.Undergrad: number of part-time undergraduates. Outstate: out-of-state tuition. Room.Board: estimated room and board cost. Books: estimated cost of books. Personal: estimated personal spending. PhD: percentage of faculty with a PhD. Terminal: percentage with terminal degrees. S.F.Ratio: student-faculty ratio. perc.alumni: percentage of alumni who donate. Expend: instructional expenditure per student. Grad.Rate: graduation rate. Analysis Objectives The primary goal of this project is to uncover underlying patterns in the structure of U.S. colleges using unsupervised learning. We aim to: Conduct univariate analysis to understand the distribution of individual features. Apply Principal Component Analysis (PCA) to reduce the dimensionality of the dataset and identify the main axes of variation. Perform clustering analysis to segment colleges into interpretable groups based on their institutional characteristics. This exploratory approach helps reveal how colleges can be grouped beyond traditional classifications, offering new perspectives on similarities and differences in the U.S. higher education system. "],["univariate-analysis.html", "2 Univariate Analysis", " 2 Univariate Analysis Private As we can see in the bar plot above, the binary variable “Private” is unbalanced, with about 73% of colleges being private and about 27% public. Applications Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GIG&quot;, &quot;Generalised Inverse Gaussian&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 8.0069136 0.0475550 168.37166 &lt; 2e-16 *** ## eta.sigma 0.3207699 0.0349293 9.18341 &lt; 2e-16 *** ## eta.nu -0.3138488 0.1163661 -2.69708 0.006995 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 3 Residual Deg. of Freedom 774 ## Global Deviance: 13834.8 ## AIC: 13840.8 ## SBC: 13854.7 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.031509, p-value = 0.4803 ## alternative hypothesis: two-sided The Apps variable shows a highly right-skewed distribution with several high-value outliers. The boxplot reveals that most institutions received a relatively low number of applications, while some colleges stand out with exceptionally high values, in some cases exceeding 20,000, with one extreme case near 50,000. The histogram with KDE estimation highlights the strong concentration in the lower range and the presence of a long right tail. The distribution fitted using the Generalized Inverse Gaussian (GIG) model provides a good approximation, capturing both the skewness and the general shape of the distribution. The estimates of \\(\\hat\\mu\\) and \\(\\hat\\sigma\\) are more significant, with values of about 8 and 0.32. The \\(\\hat\\nu\\) is slightly less significant than others parameters, with a value of about -0.31. The residual diagnostic plots suggest an adequate fit: the Q-Q plot indicates a good alignment of the residuals with the theoretical normal distribution, with only minor deviations in the tails; the residual density is approximately symmetric and centered around zero. The worm plot supports these findings: most points lie within the confidence bands, with only mild systematic deviations in the tails. This indicates that the GIG model provides a satisfactory description of the data, although it may not perfectly capture the most extreme cases. Finally, the Kolmogorov–Smirnov test (D = 0.0315, p-value = 0.4803) fails to reject the null hypothesis, further confirming the goodness of fit. Overall, the GIG distribution represents an appropriate choice to model the Applications variable. Students accepted Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;IG&quot;, &quot;Inverse Gaussian&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 7.6102623 0.0453237 167.909 &lt; 2.22e-16 *** ## eta.sigma -3.5713367 0.0253673 -140.785 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 2 Residual Deg. of Freedom 775 ## Global Deviance: 13227.8 ## AIC: 13231.8 ## SBC: 13241.1 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.046259, p-value = 0.103 ## alternative hypothesis: two-sided The Accept variable also shows a highly right-skewed distribution with several high-value outliers, similar to the Applications variable. The distribution fitted using the Inverse Gaussian (IG) model works very well. This model has two parameters, \\(\\hat\\mu\\) and \\(\\hat\\sigma\\), and both are highly significant. In the index plot, we can see that there is no dependency between rows (such as order effects). The quantile residuals are approximately distributed as \\(\\mathcal{N}(0,1),\\) therefore the model fits very well. The Kolmogorov–Smirnov test (D = 0.046259, p-value = 0.103) fails to reject the null hypothesis, further confirming the goodness of fit. Hence, the IG distribution represents an appropriate choice for modeling the Accepted variable. Students enrolled Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GIG&quot;, &quot;Generalised Inverse Gaussian&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 6.6592607 0.0463438 143.69245 &lt; 2.22e-16 *** ## eta.sigma 0.2260256 0.0524799 4.30690 1.6556e-05 *** ## eta.nu -0.7751681 0.1460976 -5.30582 1.1216e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 3 Residual Deg. of Freedom 774 ## Global Deviance: 11691.3 ## AIC: 11697.3 ## SBC: 11711.3 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.081749, p-value = 0.0008482 ## alternative hypothesis: two-sided The Enroll variable also shows a highly right-skewed distribution with several high-value outliers. However, in this case, there is a slight increase just before the value 2000, so the distribution is not monotonically decreasing. Even though the parameter estimates are significant, the Generalised Inverse Gaussian (GIG) model does not capture this small increase. This can be seen in the residual density plot (which is not symmetric) and in the worm plot (where some normal quantiles are very close to the 95% confidence interval). The Kolmogorov–Smirnov test (D = 0.081749, p-value = 0.0008482) rejects the null hypothesis. Hence, the GIG model fits well for values below 1000, but for values greater than 1000 the fit is less accurate. Top 10% Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GB1&quot;, &quot;Generalized beta type 1&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu -0.161851 0.136263 -1.18778 0.2349190 ## eta.sigma 0.454322 0.149667 3.03556 0.0024009 ** ## eta.nu -4.016184 0.611128 -6.57176 4.9724e-11 *** ## eta.tau 1.077745 0.156445 6.88895 5.6206e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 4 Residual Deg. of Freedom 773 ## Global Deviance: -789.978 ## AIC: -781.978 ## SBC: -763.356 ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.33988, p-value = 5.886e-09 ## alternative hypothesis: two-sided The Top10perc variable is highly right-skewed, with some outliers above 65%. The fitted distribution is the Generalized Beta type 1 (GB1). Most parameters are significant, with the exception of \\(\\hat\\mu\\). Diagnostic checks highlight several issues: the worm plot shows points outside the 95% confidence bands; the residual density is asymmetric; the Kolmogorov–Smirnov test (D = 0.33988, p-value = 5.886e-09) rejects the null hypothesis. Hence, the GB1 model does not adequately capture the distribution of this variable. Top 25% Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GB1&quot;, &quot;Generalized beta type 1&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 0.194144 0.289582 0.67043 0.50258 ## eta.sigma 0.200006 0.192980 1.03641 0.30001 ## eta.nu -2.153256 0.313620 -6.86582 6.6112e-12 *** ## eta.tau 1.190385 0.235046 5.06448 4.0952e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 4 Residual Deg. of Freedom 766 ## Global Deviance: -400.959 ## AIC: -392.959 ## SBC: -374.373 ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.12648, p-value = 0.1097 ## alternative hypothesis: two-sided The Top25perc distribution is more symmetric compared to Top10perc, although similar issues remain. In this case, the Generalized Beta type 1 (GB1) distribution again provides the best fit. However, the parameter \\(\\hat\\sigma\\) is not significant. From a graphical perspective, the residuals behave better than in Top10perc, and the Kolmogorov–Smirnov test fails to reject the null hypothesis, suggesting an acceptable overall fit despite some limitations. Full-time undergraduates Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GIG&quot;, &quot;Generalised Inverse Gaussian&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 8.2160631 0.0561717 146.26697 &lt; 2.22e-16 *** ## eta.sigma 0.3946007 0.0716927 5.50405 3.7115e-08 *** ## eta.nu -0.9077921 0.1186020 -7.65410 1.9540e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 3 Residual Deg. of Freedom 774 ## Global Deviance: 14046.7 ## AIC: 14052.7 ## SBC: 14066.7 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.044836, p-value = 0.1133 ## alternative hypothesis: two-sided The F.Undergrad variable is highly right-skewed with several large outliers. The Generalized Inverse Gaussian (GIG) model fits well, with all three parameters \\(\\hat\\mu, \\hat\\sigma, \\hat\\nu\\) highly significant. Residual diagnostics show: quantile residuals approximately distributed as \\(N(0,1)\\), with some fluctuations; the worm plot indicates heavier tails (S-shaped curve), suggesting excess kurtosis. The Kolmogorov–Smirnov test (D = 0.044836, p-value = 0.1133) fails to reject the null hypothesis, supporting a satisfactory fit, though not perfect in the tails. Part-time undergraduates Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;BCPEo&quot;, &quot;Box-Cox Power Exponential-orig.&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 5.8169964 0.0635828 91.48690 &lt; 2.22e-16 *** ## eta.sigma 0.4616539 0.0245371 18.81456 &lt; 2.22e-16 *** ## eta.nu 0.1008651 0.0173222 5.82287 5.7845e-09 *** ## eta.tau 0.8734533 0.0806019 10.83663 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 4 Residual Deg. of Freedom 773 ## Global Deviance: 11782 ## AIC: 11790 ## SBC: 11808.6 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.14887, p-value = 2.545e-11 ## alternative hypothesis: two-sided The P.Undergrad variable is extremely concentrated near zero and highly right-skewed with large outliers. The selected model is the Box-Cox Power Exponential-original (BCPEo). All parameters \\(\\theta = (\\hat\\mu, \\hat\\sigma, \\hat\\nu, \\hat\\tau)\\) are significant. However, diagnostic checks reveal problems: the residual density is asymmetric; the Kolmogorov–Smirnov test (D = 0.14887, p-value = 2.545e-11) rejects the null hypothesis. Thus, despite parameter significance, the BCPEo distribution does not provide an adequate fit. Out-of-state tuition Descriptive statistics Distribution Fitting ## Gamma mixture model with 2 components ## comp1 comp2 ## pi 0.9126484 8.735160e-02 ## mu 9732.5083079 1.783953e+04 ## sd 3496.9897502 1.628513e+03 ## shape 7.7457031 1.200008e+02 ## rate 0.0007959 6.726700e-03 ## ## EM iterations: 216 AIC: 15016.83 BIC: 15040.11 log-likelihood: -7503.42 The Outstate variable is only mildly right-skewed. The KDE reveals a structure that suggests a mixture distribution is appropriate. Therefore, a mixture of two Gamma distributions was selected, which successfully captures the observed pattern. Room and board cost Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GA&quot;, &quot;Gamma&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 8.37965953 0.00898582 932.5425 &lt; 2.22e-16 *** ## eta.sigma -1.38438694 0.02510634 -55.1409 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 2 Residual Deg. of Freedom 775 ## Global Deviance: 13042.7 ## AIC: 13046.7 ## SBC: 13056 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.052409, p-value = 0.09587 ## alternative hypothesis: two-sided The Room.Board variable shows a mild right skew. The Gamma distribution provides a reasonably good fit: parameters \\((\\hat\\mu, \\hat\\sigma)\\) are significant; residual diagnostics are satisfactory, though minor imperfections appear in the upper tail and in the quantile residuals. The Kolmogorov–Smirnov test (D = 0.052409, p-value = 0.09587) fails to reject the null hypothesis, supporting the adequacy of the Gamma model. Cost of books Descriptive statistics Distribution Fitting ## Gamma mixture model with 2 components ## comp1 comp2 ## pi 0.8860079 0.1139921 ## mu 536.3065083 651.0025872 ## sd 100.0155646 368.3087167 ## shape 28.7535156 3.1242187 ## rate 0.0536140 0.0047991 ## ## EM iterations: 33 AIC: 9752.47 BIC: 9775.75 log-likelihood: -4871.24 The behavior of the variable Books is unusual. The variable also shows a right-skewed distribution with several high- and low-value outliers. The KDE exhibits overfitting, which occurs when the smoothing parameter is too small. The fitDist function in GAMLSS does not work well in this case, so a mixture of two Gamma distributions was applied. Personal spending Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;LOGNO&quot;, &quot;Log Normal&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 7.0850691 0.0174117 406.9153 &lt; 2.22e-16 *** ## eta.sigma -0.7228953 0.0253673 -28.4971 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 2 Residual Deg. of Freedom 775 ## Global Deviance: 12091.8 ## AIC: 12095.8 ## SBC: 12105.2 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.21876, p-value = 1.202e-12 ## alternative hypothesis: two-sided The Personal variable is right-skewed with outliers. The Lognormal (LOGNO) distribution was tested but performed poorly: residual diagnostics deviate from normality; the Kolmogorov–Smirnov test confirms lack of fit. Thus, the LOGNO model is not adequate for this variable. Percentage of PhD Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GB1&quot;, &quot;Generalized beta type 1&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 1.913129 0.465327 4.11137 3.9332e-05 *** ## eta.sigma -1.135873 0.220442 -5.15270 2.5677e-07 *** ## eta.nu 12.748402 4.141176 3.07845 0.0020808 ** ## eta.tau -13.549139 4.134993 -3.27670 0.0010503 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 4 Residual Deg. of Freedom 769 ## Global Deviance: -746.751 ## AIC: -738.751 ## SBC: -720.15 ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.27364, p-value = 1.609e-05 ## alternative hypothesis: two-sided The PhD variable was fitted using the Generalized Beta type 1 (GB1). However, the model does not fit well, as shown by residual diagnostics and the Kolmogorov–Smirnov test. Moreover, the fitting procedure raised convergence warnings, indicating potential instability in parameter estimation. Percentage of terminal degrees Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;BE&quot;, &quot;Beta&quot;) ## ## Call: gamlssML(formula = y, family = BE) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 1.3457668 0.0318862 42.2053 &lt; 2.22e-16 *** ## eta.sigma -0.5809269 0.0339839 -17.0942 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 2 Residual Deg. of Freedom 761 ## Global Deviance: -1010.92 ## AIC: -1006.92 ## SBC: -997.65 ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.30976, p-value = 5.949e-06 ## alternative hypothesis: two-sided The Terminal variable was modeled with the Beta (BE) distribution. Although the parameter estimates are reasonable, residual diagnostics show values outside the 95% confidence interval and the Kolmogorov–Smirnov test rejects the null hypothesis. Hence, the BE model is not a suitable fit. Student-faculty ratio Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GB1&quot;, &quot;Generalized beta type 1&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu -0.542462 0.177567 -3.05496 0.0022509 ** ## eta.sigma 0.275956 0.246668 1.11873 0.2632547 ## eta.nu -11.963796 2.635678 -4.53917 5.6475e-06 *** ## eta.tau 1.860055 0.200696 9.26803 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 4 Residual Deg. of Freedom 773 ## Global Deviance: -2857.75 ## AIC: -2849.75 ## SBC: -2831.13 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.17268, p-value = 6.609e-05 ## alternative hypothesis: two-sided The S.F.Ratio variable was modeled using Generalized beta type 1 (GB1), but both residual analysis and the Kolmogorov–Smirnov test show poor fit. This indicates that the GB1 distribution does not adequately describe the data. Percentage of student donors Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;BEo&quot;, &quot;Beta original&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 0.8757280 0.0478023 18.3198 &lt; 2.22e-16 *** ## eta.sigma 2.0962922 0.0514518 40.7428 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 2 Residual Deg. of Freedom 773 ## Global Deviance: -1151.36 ## AIC: -1147.36 ## SBC: -1138.05 ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.26808, p-value = 0.0002684 ## alternative hypothesis: two-sided The perc.alumni variable was modeled with the Beta-original (BEo) distribution. Although the KDE and fitted distribution show some alignment, diagnostic checks reveal important issues: quantile residuals exhibit kurtosis; the Kolmogorov–Smirnov test (D = 0.26808, p-value = 0.0002684) rejects the null hypothesis. Thus, the BEo distribution provides only a partial description of the data. Instructional expenditure Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;BCPE&quot;, &quot;Box-Cox Power Exponential&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu 8410.5398898 116.6109377 72.12479 &lt; 2.22e-16 *** ## eta.sigma -0.9622819 0.0307236 -31.32062 &lt; 2.22e-16 *** ## eta.nu -0.5226112 0.0815880 -6.40549 1.4989e-10 *** ## eta.tau 0.4273583 0.0771043 5.54260 2.9802e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 4 Residual Deg. of Freedom 773 ## Global Deviance: 14843 ## AIC: 14851 ## SBC: 14869.6 ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.026043, p-value = 0.6939 ## alternative hypothesis: two-sided The Expend variable is well modeled by the Box-Cox Power Exponential (BCPE). All parameters are highly significant, residuals follow approximately a \\(N(0,1)\\) distribution, and the Kolmogorov–Smirnov test (D = 0.026043, p-value = 0.6939) fails to reject the null hypothesis. This indicates a very good fit. Graduation rate Descriptive statistics Distribution Fitting ## ******************************************************************* ## Family: c(&quot;GB1&quot;, &quot;Generalized beta type 1&quot;) ## ## Call: gamlssML(formula = y, family = DIST[i]) ## ## Fitting method: &quot;nlminb&quot; ## ## ## Coefficient(s): ## Estimate Std. Error t value Pr(&gt;|t|) ## eta.mu -0.380954 0.194370 -1.95994 0.0500029 . ## eta.sigma 0.400045 0.141763 2.82193 0.0047736 ** ## eta.nu -1.986350 0.384539 -5.16554 2.3974e-07 *** ## eta.tau 1.810241 0.201202 8.99712 &lt; 2.22e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Degrees of Freedom for the fit: 4 Residual Deg. of Freedom 762 ## Global Deviance: -602.839 ## AIC: -594.839 ## SBC: -576.274 ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: unique(variable) ## D = 0.19321, p-value = 0.004664 ## alternative hypothesis: two-sided The Grad.Rate variable is slightly left-skewed. The Generalized beta type 1 (GB1) distribution provides a reasonable graphical fit, with residuals showing no major problems. However, the Kolmogorov–Smirnov test (D = 0.19321, p-value = 0.004664) rejects the null hypothesis, suggesting that the GB1 model does not fully capture the distribution. "],["dimensionality-reduction.html", "3 Dimensionality Reduction", " 3 Dimensionality Reduction The exploratory analysis of the relationships among the variables highlights a strong presence of both positive and negative correlations. In particular, very close relationships are observed among Apps, Accept, and Enroll, which represent subsequent steps of the admission process, and between Top10perc and Top25perc, both measuring the academic quality of incoming students. Furthermore, Terminal and PhD appear to be almost perfectly correlated, while a significant negative correlation emerges between Expend and S.F.Ratio, suggesting that universities with higher expenses tend to have a lower student-to-faculty ratio. These findings reveal a considerable degree of redundancy in the data, thus making dimensionality reduction techniques such as PCA particularly suitable. PCA (Principal Component Analysis) From the first biplot, we can see that Apps, Accept, Enroll, and F.Undergrad are the variables for which the first two principal components capture most of the variance. These variables are correlated, as indicated by the small angles between their arrows, which is confirmed by the correlation matrix. PC1 captures most of the variance of F.Undergrad and part of P.Undergrad, while PC2 captures some of the variance of Top25perc and Top10perc. In the last plot, we can see that the second dimension separates the Private attribute quite well. Eigenvectors This table is useful to understand the orientation of the variables. A negative eigenvector value means that higher values of that variable are associated with lower values along the corresponding Principal Component axis in the transformed PCA space. PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15 PC16 PC17 Apps 0.25 -0.33 0.06 0.28 -0.01 -0.02 0.04 0.10 0.09 0.05 -0.04 -0.02 -0.60 0.08 -0.13 0.46 0.36 Accept 0.21 -0.37 0.10 0.27 -0.06 0.01 0.01 0.06 0.18 0.04 0.06 0.15 -0.29 0.03 0.15 -0.52 -0.54 Enroll 0.18 -0.40 0.08 0.16 0.06 -0.04 0.03 -0.06 0.13 0.03 0.07 -0.01 0.44 -0.09 -0.03 -0.40 0.61 Top10perc 0.35 0.08 -0.04 -0.05 0.40 -0.05 0.16 0.12 -0.34 0.06 0.01 -0.04 0.00 -0.11 -0.70 -0.15 -0.14 Top25perc 0.34 0.04 0.02 -0.11 0.43 0.03 0.12 0.10 -0.40 0.01 0.27 0.09 -0.02 0.15 0.62 0.05 0.08 F.Undergrad 0.15 -0.42 0.06 0.10 0.04 -0.04 0.03 -0.08 0.06 0.02 0.08 -0.06 0.52 -0.06 -0.01 0.56 -0.41 P.Undergrad 0.03 -0.32 -0.14 -0.16 -0.30 -0.19 -0.06 -0.57 -0.56 -0.22 -0.10 0.06 -0.13 0.02 -0.02 -0.05 0.01 Outstate 0.29 0.25 -0.05 0.13 -0.22 -0.03 -0.11 -0.01 0.00 0.19 -0.14 0.82 0.14 -0.03 -0.04 0.10 0.05 Room.Board 0.25 0.14 -0.15 0.18 -0.56 0.16 -0.21 0.22 -0.28 0.30 0.36 -0.35 0.07 -0.06 0.00 -0.03 0.00 Books 0.06 -0.06 -0.68 0.09 0.13 0.64 0.15 -0.21 0.13 -0.08 -0.03 0.03 -0.01 -0.07 0.01 0.00 0.00 Personal -0.04 -0.22 -0.50 -0.23 0.22 -0.33 -0.63 0.23 0.09 0.14 0.02 0.04 -0.04 0.03 0.00 -0.01 0.00 PhD 0.32 -0.06 0.13 -0.53 -0.14 0.09 0.00 0.08 0.19 -0.12 -0.04 -0.02 -0.13 -0.69 0.11 0.03 0.01 Terminal 0.32 -0.05 0.07 -0.52 -0.20 0.15 0.03 0.01 0.25 -0.09 0.06 -0.02 0.06 0.67 -0.16 -0.03 0.01 S.F.Ratio -0.18 -0.25 0.29 -0.16 0.08 0.49 -0.22 0.08 -0.27 0.47 -0.45 0.01 0.02 0.04 0.02 -0.02 0.00 perc.alumni 0.21 0.25 0.15 0.02 0.22 -0.05 -0.24 -0.68 0.26 0.42 0.13 -0.18 -0.10 -0.03 0.01 0.00 -0.02 Expend 0.32 0.13 -0.23 0.08 -0.08 -0.30 0.23 0.05 0.05 0.13 -0.69 -0.33 0.09 0.07 0.23 -0.04 -0.04 Grad.Rate 0.25 0.17 0.21 0.27 0.11 0.22 -0.56 0.01 -0.04 -0.59 -0.22 -0.12 0.07 0.04 0.00 -0.01 -0.01 Percentage of variance in each principal component explained by the variables Remember: each column is scaled by the variance captured by its component (the eigenvalue). For example, in PC17 the variable Enroll explains 37.17% of the variance of that component. However, PC17 itself captures only 0.14% of the total variance. PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15 PC16 PC17 Apps 6.19% 11% 0.4% 7.91% 0% 0.03% 0.18% 1.06% 0.81% 0.28% 0.19% 0.06% 35.5% 0.65% 1.78% 21.08% 12.89% Accept 4.31% 13.85% 1.03% 7.17% 0.31% 0.01% 0.02% 0.32% 3.16% 0.17% 0.34% 2.11% 8.56% 0.11% 2.12% 26.89% 29.53% Enroll 3.11% 16.3% 0.69% 2.62% 0.31% 0.18% 0.08% 0.34% 1.65% 0.12% 0.48% 0.01% 19.77% 0.73% 0.09% 16.35% 37.17% Top10perc 12.55% 0.68% 0.12% 0.27% 15.64% 0.28% 2.6% 1.5% 11.63% 0.41% 0.01% 0.15% 0% 1.16% 48.68% 2.21% 2.1% Top25perc 11.83% 0.2% 0.06% 1.2% 18.19% 0.11% 1.4% 1.05% 16.3% 0.02% 7.46% 0.8% 0.05% 2.3% 38.1% 0.27% 0.65% F.Undergrad 2.39% 17.45% 0.38% 1.01% 0.19% 0.19% 0.06% 0.62% 0.35% 0.04% 0.66% 0.32% 27.42% 0.32% 0.01% 31.4% 17.2% P.Undergrad 0.07% 9.93% 1.95% 2.51% 9.14% 3.66% 0.37% 32.58% 31.44% 4.98% 1.01% 0.4% 1.59% 0.04% 0.04% 0.28% 0.01% Outstate 8.69% 6.23% 0.22% 1.72% 4.95% 0.09% 1.18% 0.01% 0% 3.48% 2.05% 67.81% 2.01% 0.12% 0.15% 1.03% 0.26% Room.Board 6.2% 1.9% 2.22% 3.42% 31.46% 2.65% 4.4% 4.9% 7.56% 8.9% 12.91% 12.57% 0.49% 0.34% 0% 0.07% 0% Books 0.42% 0.32% 45.89% 0.76% 1.62% 41.1% 2.24% 4.55% 1.79% 0.67% 0.1% 0.08% 0.01% 0.45% 0.01% 0% 0% Personal 0.18% 4.84% 24.97% 5.32% 4.94% 10.98% 40.17% 5.41% 0.89% 1.85% 0.03% 0.15% 0.16% 0.08% 0% 0.02% 0% PhD 10.13% 0.34% 1.61% 28.59% 1.96% 0.83% 0% 0.59% 3.43% 1.52% 0.16% 0.05% 1.63% 47.77% 1.26% 0.09% 0.02% Terminal 10.05% 0.22% 0.44% 26.98% 4.19% 2.4% 0.08% 0.01% 6.5% 0.78% 0.35% 0.03% 0.34% 45.03% 2.53% 0.07% 0% S.F.Ratio 3.13% 6.08% 8.4% 2.6% 0.63% 23.72% 4.81% 0.7% 7.54% 22.28% 19.8% 0.01% 0.03% 0.17% 0.04% 0.05% 0% perc.alumni 4.21% 6.08% 2.16% 0.03% 4.68% 0.22% 5.92% 46.04% 6.52% 17.89% 1.71% 3.34% 1.08% 0.07% 0.01% 0% 0.04% Expend 10.17% 1.73% 5.14% 0.63% 0.58% 8.89% 5.13% 0.29% 0.24% 1.75% 47.9% 10.63% 0.88% 0.53% 5.19% 0.19% 0.12% Grad.Rate 6.37% 2.86% 4.33% 7.24% 1.19% 4.67% 31.35% 0% 0.18% 34.84% 4.83% 1.49% 0.48% 0.13% 0% 0% 0.02% Percentage of variance of variables explained by each PC PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15 PC16 PC17 Apps 33.69% 49.24% 0.47% 7.97% 0% 0.02% 0.11% 0.62% 0.43% 0.11% 0.06% 0.01% 5.95% 0.09% 0.16% 0.77% 0.3% Accept 23.46% 62% 1.2% 7.22% 0.29% 0% 0.01% 0.19% 1.68% 0.07% 0.11% 0.46% 1.44% 0.02% 0.19% 0.99% 0.68% Enroll 16.92% 72.99% 0.81% 2.64% 0.29% 0.15% 0.05% 0.2% 0.88% 0.05% 0.15% 0% 3.31% 0.11% 0.01% 0.6% 0.85% Top10perc 68.32% 3.04% 0.14% 0.27% 14.59% 0.24% 1.57% 0.88% 6.17% 0.17% 0% 0.03% 0% 0.17% 4.28% 0.08% 0.05% Top25perc 64.42% 0.9% 0.07% 1.21% 16.97% 0.09% 0.85% 0.62% 8.64% 0.01% 2.34% 0.18% 0.01% 0.33% 3.35% 0.01% 0.01% F.Undergrad 13.02% 78.12% 0.44% 1.02% 0.18% 0.16% 0.04% 0.37% 0.19% 0.02% 0.21% 0.07% 4.59% 0.05% 0% 1.15% 0.4% P.Undergrad 0.38% 44.46% 2.29% 2.53% 8.53% 3.1% 0.23% 19.13% 16.66% 2.01% 0.32% 0.09% 0.27% 0.01% 0% 0.01% 0% Outstate 47.29% 27.91% 0.25% 1.74% 4.62% 0.08% 0.71% 0.01% 0% 1.41% 0.64% 14.94% 0.34% 0.02% 0.01% 0.04% 0.01% Room.Board 33.76% 8.5% 2.6% 3.45% 29.36% 2.24% 2.66% 2.88% 4.01% 3.59% 4.04% 2.77% 0.08% 0.05% 0% 0% 0% Books 2.28% 1.42% 53.83% 0.76% 1.51% 34.82% 1.36% 2.67% 0.95% 0.27% 0.03% 0.02% 0% 0.06% 0% 0% 0% Personal 0.98% 21.66% 29.3% 5.36% 4.61% 9.31% 24.3% 3.18% 0.47% 0.75% 0.01% 0.03% 0.03% 0.01% 0% 0% 0% PhD 55.16% 1.52% 1.89% 28.79% 1.83% 0.71% 0% 0.35% 1.82% 0.62% 0.05% 0.01% 0.27% 6.87% 0.11% 0% 0% Terminal 54.72% 0.97% 0.51% 27.17% 3.91% 2.03% 0.05% 0.01% 3.44% 0.32% 0.11% 0.01% 0.06% 6.47% 0.22% 0% 0% S.F.Ratio 17.05% 27.24% 9.86% 2.62% 0.59% 20.1% 2.91% 0.41% 3.99% 9% 6.2% 0% 0.01% 0.02% 0% 0% 0% perc.alumni 22.89% 27.23% 2.53% 0.03% 4.37% 0.19% 3.58% 27.03% 3.45% 7.22% 0.53% 0.74% 0.18% 0.01% 0% 0% 0% Expend 55.36% 7.77% 6.03% 0.63% 0.54% 7.53% 3.11% 0.17% 0.13% 0.71% 14.99% 2.34% 0.15% 0.08% 0.46% 0.01% 0% Grad.Rate 34.66% 12.83% 5.08% 7.29% 1.11% 3.96% 18.97% 0% 0.09% 14.07% 1.51% 0.33% 0.08% 0.02% 0% 0% 0% In conclusion, there are some variables are strongly associated with the first two components (e.g., Apps, Accept, Enroll, F.Undergrad), while others load more heavily on different dimensions (e.g., Books on PC3 and PC6, Personal on PC3 and PC7). This suggests that the first components capture most of the information related to student admission and enrollment processes, while later components capture variability associated with more specific factors, such as expenditures, personal costs, or academic staff ratios. Choosing the number of PCs Let’s apply Kaiser’s rule The PC four is on the line and the variable that it explain better are inside the first PC. Then we select only the first 3 components: PC1 – Academic Prestige and Student Spending: This component captures the level of academic prestige and quality of a college. High values indicate selective institutions with good resources and qualified faculty. PC2 – Size and Enrollment Volume: This component appears to reflect the size of the institution and the scale of its admissions process. Low values correspond to large, highly attended colleges. PC3 – Personal and Book Expenses: Personal and Book Expenses: A component that seems to reflect student-related out-of-pocket expenses. Negative values indicate higher costs for personal living and study materials. "],["cluster-analysis.html", "4 Cluster Analysis 4.1 Feasibility of the cluster analysis 4.2 Hard clustering 4.3 Soft Clustering", " 4 Cluster Analysis This chapter focuses on clustering analysis, with the aim of identifying pattern in the data. The adopted approach is structured into multiple stages, each designed to evaluate, through different methods, the feasibility and robustness of clustering. 4.1 Feasibility of the cluster analysis This first part investigates the feasibility of clustering by means of several exploratory techniques. Specifically, scatter plots of the variables colored according to the binary variable Private are compared with scatter plots generated from random values, in order to assess the presence of non-random patterns. The analysis is then extended to the first two principal components obtained via PCA. To further explore the correlation structure among observations, the VAT (Visual Assessment of Tendency) algorithm is applied, and the Hopkins statistic is computed to measure the clustering tendency of the dataset. Both the VAT algorithm and the Hopkins statistic (0.1389425) indicate the presence of clusters in the dataset. 4.2 Hard clustering This part is devoted to the application of hard clustering algorithms. 4.2.1 Hierarchical clustering Before implementing hierarchical clustering, it is useful to compute the correlation between the cophenetic distances and the original distance matrix for each combination of distance and linkage method. Type Correlations euclidean_single 0.7038 euclidean_complete 0.6547 euclidean_average 0.7987 euclidean_ward.D2 0.4471 manhattan_single 0.6272 manhattan_complete 0.5267 manhattan_average 0.7605 manhattan_ward.D2 0.4633 The combinations with the highest cophenetic correlations are euclidean_average and manhattan_average. For both, a dendrogram is constructed, the optimal number of clusters (k) is identified, and the tree is pruned at this level. Hierarchical clustering with euclidean distance and average linked method NbClust function ## [1] &quot;Frey index : No clustering structure in this data set&quot; ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 8 proposed 2 as the best number of clusters ## * 4 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 7 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 1 proposed 9 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* Two clusters ## cluster size ave.sil.width ## 1 1 776 0.68 ## 2 2 1 0.00 Five clusters ## cluster size ave.sil.width ## 1 1 766 0.48 ## 2 2 8 0.51 ## 3 3 1 0.00 ## 4 4 1 0.00 ## 5 5 1 0.00 Based on the results, the hierarchical clusterings with euclidean distance and the average linkage method, for \\(k = 2\\) or \\(k = 5\\), do not yield meaningful results. Hierarchical clustering with manhattan distance and average linked method NbClust function ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 11 proposed 2 as the best number of clusters ## * 1 proposed 3 as the best number of clusters ## * 1 proposed 5 as the best number of clusters ## * 2 proposed 6 as the best number of clusters ## * 1 proposed 7 as the best number of clusters ## * 6 proposed 8 as the best number of clusters ## * 2 proposed 10 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* Two clusters ## cluster size ave.sil.width ## 1 1 776 0.53 ## 2 2 1 0.00 Eight clusters ## cluster size ave.sil.width ## 1 1 604 0.28 ## 2 2 90 0.27 ## 3 3 70 0.19 ## 4 4 8 0.30 ## 5 5 2 0.49 ## 6 6 1 0.00 ## 7 7 1 0.00 ## 8 8 1 0.00 Based on the results, the hierarchical clusterings with manhattan distance and the average linkage method, for k = 2 or k = 8, do not yield meaningful results. 4.2.2 Partional clustering Now, let us apply algorithms of partitioning clustering, such as k-means and k-medoids. We also apply k-means++, which selects the initial centroids in a way that reduces the probability of choosing centroids that are too close to each other. We observe that, for \\(k = 2\\), k-means++ successfully identifies the separation between private and public colleges, unlike the classic k-means. However, for \\(k = 3\\), the behavior is essentially the same. K-means NbClust function ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 4 proposed 2 as the best number of clusters ## * 15 proposed 3 as the best number of clusters ## * 1 proposed 7 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 1 proposed 9 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* Three clusters ## cluster size ave.sil.width ## 1 1 93 0.18 ## 2 2 246 0.23 ## 3 3 438 0.26 Classic k-means with \\(k = 3\\) performs better than hierarchical clustering, but it is still difficult to clearly distinguish three separate clusters. It should also be noted that the total variance explained by the first two principal components is relatively low. The silhouette plot shows that the elements within each cluster do not achieve a high silhouette score. K-means++ Two clusters ## cluster size ave.sil.width ## 1 1 676 0.36 ## 2 2 101 0.20 ## true_labels ## pred_labelsKmeans No Yes ## No 88 13 ## Yes 124 552 The k-means++, with some errors, is able to identify the private and the public college and the average silhouette width has a better score than others algorithms. K-medoids ## cluster size ave.sil.width ## 1 1 388 0.27 ## 2 2 237 0.23 ## 3 3 152 0.10 ## cluster size ave.sil.width ## 1 1 367 0.32 ## 2 2 236 0.22 ## 3 3 174 0.11 The K-medoids algorithm is more robust to outliers, and when using the Manhattan distance instead of the Euclidean distance, it yields better results in terms of the average silhouette width. 4.2.3 Density-based clustering Density-based clustering offers an alternative approach to partitioning methods, as it does not require the number of clusters to be specified in advance. The underlying idea is that clusters correspond to high-density regions in the data space, separated by low-density areas. In this section, two main algorithms are applied: DBSCAN (Density-Based Spatial Clustering of Applications with Noise), which identifies groups of points with sufficient density and distinguishes noise points, and OPTICS (Ordering Points To Identify the Clustering Structure), which extends DBSCAN by enabling the detection of clusters with varying densities. DBSCAN Setting parameters for DBSCAN | From Documentation of dbscan package function dbscan The parameters minPts and eps define the minimum density required in the area around core points which form the backbone of clusters. minPts is the number of points required in the neighborhood around the point defined by the parameter eps (i.e., the radius around the point). Both parameters depend on each other and changing one typically requires changing the other one as well. The parameters also depend on the size of the data set with larger datasets requiring a larger minPts or a smaller eps. ⁠minPts:⁠ The original DBSCAN paper (Ester et al, 1996) suggests to start by setting \\(minPts \\geq d + 1\\), the data dimensionality plus one or higher with a minimum of 3. Larger values are preferable since increasing the parameter suppresses more noise in the data by requiring more points to form clusters. Sander et al (1998) uses in the examples two times the data dimensionality. Note that setting \\(minPts \\leq 2\\) is equivalent to hierarchical clustering with the single link metric and the dendrogram cut at height eps. ⁠eps:⁠ A suitable neighborhood size parameter eps given a fixed value for minPts can be found visually by inspecting the kNNdistplot() of the data using \\(k=minPts−1\\) (minPts includes the point itself, while the k-nearest neighbors distance does not). The k-nearest neighbor distance plot sorts all data points by their k-nearest neighbor distance. A sudden increase of the kNN distance (a knee) indicates that the points to the right are most likely outliers. Choose eps for DBSCAN where the knee is. Here, the elbow method is somewhat difficult to interpret because there is no clear gap, and the resulting plot (on the right) is not satisfactory. Optics This implementation of OPTICS implements the original algorithm as described by Ankerst et al (1999). OPTICS is an ordering algorithm with methods to extract a clustering from the ordering. OPTICS linearly orders the data points such that points which are spatially closest become neighbors in the ordering. The closest analog to this ordering is dendrogram in single-link hierarchical clustering. Extracting a clustering | From Documentation of dbscan package function optics Several methods to extract a clustering from the order returned by OPTICS are implemented: extractDBSCAN() extracts a clustering from an OPTICS ordering that is similar to what DBSCAN would produce with an eps set to eps_cl (see Ankerst et al, 1999). The only difference to a DBSCAN clustering is that OPTICS is not able to assign some border points and reports them instead as noise. extractXi() extract clusters hierarchically specified in Ankerst et al (1999) based on the steepness of the reachability plot. One interpretation of the xi parameter is that it classifies clusters by change in relative cluster density. The used algorithm was originally contributed by the ELKI framework and is explained in Schubert et al (2018), but contains a set of fixes. extractDBSCAN() with eps_cl=1.61 extractXi() with xi=0.01 Finally, both DBSCAN and OPTICS yield unsatisfactory results. 4.2.4 Cluster validation 4.2.4.1 External clustering validation This is possible apply only on the result of K-means++ (\\(k=2\\)), because it is the unique clustering algorithem that identify the beahiour of the private and public college. Metric Value Corrected Rand Index 0.3447 Meila’s VI Index 0.7293 Meila’s VI Index is not very high (its range is \\((0, \\infty)\\)), but the Corrected Rand Index is low. 4.2.4.2 Stability measures Method APN AD ADM FOM HC - Euclidean - Avg - k=2 0.0003 5.3248 0.0075 0.9988 HC - Euclidean - Avg - k=5 0.0090 5.1870 0.1195 0.9812 HC - Manhattan - Avg - k=2 0.0100 17.0750 0.1067 0.9968 HC - Manhattan - Avg - k=8 0.0762 14.6278 0.7625 0.8614 KMeans - k=3 0.0446 4.3166 0.1900 0.8142 KMeans++ - k=2 0.2790 5.1583 1.4171 0.9792 KMedoids - Euclidean - k=3 0.1047 4.3950 0.4561 0.8357 KMedoids - Manhattan - k=3 0.1116 13.6128 0.4865 0.8384 ## Best APN: &quot;HC - Euclidean - Avg - k=2&quot; ## Best AD: &quot;KMeans - k=3&quot; ## Best ADM: &quot;HC - Euclidean - Avg - k=2&quot; ## Best FOM: &quot;KMeans - k=3&quot; From these stability measures, we can see that the most stable clustering algorithms are HC with Euclidean distance and average linkage (\\(k=2\\)), and K-means (\\(k=3\\)). However, the first solution yields only two clusters (with one of them has only one element). Therefore, K-means with \\(k=3\\) is preferable. Finally, we should choose between k-means with \\(k=3\\) and k-means++ with \\(k=2\\). 4.3 Soft Clustering This section is devoted to the application of soft clustering approaches, such as fuzzy clustering and model-based clustering. 4.3.1 Fuzzy Clustering Here we compare the results of FKMEN with those of FKMedN. Other algorithms, such as GK-FKM and GKB-FK, were also tested, but they required too much computation time to provide results. Fuzzy k-Means with Entropy regularization and Noise cluster To select the optimal value of \\(k\\), we rely on fuzziness measures such as: Partition Coefficient (PC): ## PC k=2 PC k=3 PC k=4 PC k=5 PC k=6 ## 0.7949485 0.8783218 0.8910608 0.8914083 0.8325891 Best value at 5 Modified Partition Coefficient (MPC) ## MPC k=2 MPC k=3 MPC k=4 MPC k=5 MPC k=6 ## 0.5898970 0.8174826 0.8547477 0.8642603 0.7991070 Best value at 5 Partition Entropy (PE) ## PE k=2 PE k=3 PE k=4 PE k=5 PE k=6 ## 0.07165039 0.08514369 0.10977413 0.13535170 0.23454366 Best value at 2 and on compactness/separation measures such as: Xie and Beni (XB) index ## XB k=2 XB k=3 XB k=4 XB k=5 XB k=6 ## 0.4991835 0.5150927 0.7636770 0.8387996 1.3164018 Best value at 2 Fuzzy Silhouette (FS) index ## SIL.F k=2 SIL.F k=3 SIL.F k=4 SIL.F k=5 SIL.F k=6 ## 0.3518378 0.3735748 0.3156925 0.2980328 0.2237030 Best value at 3 According to these criteria, the optimal number of clusters is \\(k=2\\), since the Fuzzy Silhouette (FS) index reaches the second highest value at \\(k=2\\). Therefore, we apply FKMEN with \\(k=2\\). The resulting clusters have the following characteristics: ## Clus 1 Clus 2 ## 334 443 The centroids are given by: Apps Accept Enroll Top10perc Top25perc F.Undergrad P.Undergrad Outstate Room.Board Books Personal PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate Clus 1 -0.215 -0.205 -0.313 0.472 0.553 -0.358 -0.329 0.798 0.570 -0.095 -0.422 0.538 0.565 -0.426 0.671 0.333 0.624 Clus 2 -0.326 -0.310 -0.265 -0.565 -0.578 -0.237 -0.097 -0.592 -0.489 -0.173 0.068 -0.542 -0.575 0.295 -0.421 -0.491 -0.464 The confusion matrix is: ## ## 1 2 ## No 32 180 ## Yes 299 266 where the Rand index has value 0.0490135. FK-Med algorithm with Noise cluster (FK MedN algorithm) By Fuzzy Silhouette (FS) index: ## SIL.F k=2 SIL.F k=3 SIL.F k=4 SIL.F k=5 SIL.F k=6 ## 0.31672170 0.25337145 0.22778222 0.18049682 0.04819403 The best value is 2. Therefore, we apply FKMedN with \\(k=2\\). The resulting clusters have the following characteristics: ## Clus 1 Clus 2 ## 378 399 The centroids are given by: Apps Accept Enroll Top10perc Top25perc F.Undergrad P.Undergrad Outstate Room.Board Books Personal PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate Clus 1 -0.516 -0.496 -0.582 -0.258 0.364 -0.575 -0.528 0.000 -0.463 -0.148 0.162 -0.592 -0.523 -0.578 -0.221 -0.329 -0.085 Clus 2 -0.200 -0.093 -0.369 -0.032 -0.091 -0.462 -0.405 0.742 0.062 -0.602 -0.208 0.756 0.632 -0.275 0.263 -0.049 0.381 The confusion matrix is: ## ## 1 2 ## No 105 107 ## Yes 273 292 where the Rand index has value -6.9482421^{-4}. 4.3.2 Model Based Clustering One disadvantage of traditional clustering methods, such as hierarchical and partitioning clustering algorithms, is that they are largely heuristic and not based on formal models. An alternative is model-based clustering that use probabilistic assumptions. ## Best BIC values: ## VVE,9 VVE,8 VVE,7 ## BIC -18547.17 -18671.4418 -18925.0659 ## BIC diff 0.00 -124.2717 -377.8958 Using the Mclust function, the best model selected is VVE, which corresponds to clusters with variable volume and shape but equal orientation, with \\(k=9\\). ## ---------------------------------------------------- ## Gaussian finite mixture model fitted by EM algorithm ## ---------------------------------------------------- ## ## Mclust VVE (ellipsoidal, equal orientation) model with 9 components: ## ## log-likelihood n df BIC ICL ## -7776.111 777 450 -18547.17 -18669.83 ## ## Clustering table: ## 1 2 3 4 5 6 7 8 9 ## 52 101 90 125 112 94 46 54 103 However, from a graphical point of view, the result is very difficult to interpret. "],["conclusion.html", "5 Conclusion", " 5 Conclusion In conclusion, the univariate analysis showed that not all fitted models are suitable for every variable. Regarding the College dataset, the application of PCA is not entirely appropriate since the variables with the strongest correlations represent subsequent stages of the admissions process (such as Apps, Accept, and Enroll). The first two principal components alone are insufficient: to reach 80% of explained variance, at least six components are required. However, by applying Kaiser’s rule, it is possible to reduce the dimensionality of the dataset to three meaningful principal components: PC1 – Academic Prestige and Student Spending: Captures the level of academic prestige and quality of a college. Higher values correspond to selective institutions with strong resources and qualified faculty. PC2 – Size and Enrollment Volume: Reflects the size of the institution and the scale of its admissions process. Lower values indicate larger colleges with high enrollment volumes. PC3 – Personal and Book Expenses: Represents student-related out-of-pocket costs. Negative values indicate higher personal living and study material expenses. Finally, the clustering analysis revealed that the most effective algorithm was k-means++, which identified two clusters that aligned well with the distinction between private and public colleges. The classic k-means with three clusters, however, uncovered additional patterns: Cluster 1 (blue): Located at the lower end of PC2, grouping larger institutions with high enrollment volumes, but not necessarily characterized by high academic prestige. Cluster 2 (yellow): Shows higher values on PC1, representing selective and prestigious universities with better resources and faculty quality. Cluster 3 (gray): Positioned closer to the origin, representing medium-sized institutions with intermediate levels of prestige. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
