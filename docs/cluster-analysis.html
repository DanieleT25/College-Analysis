<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Cluster Analysis | College analysis</title>
  <meta name="description" content="4 Cluster Analysis | College analysis" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Cluster Analysis | College analysis" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Cluster Analysis | College analysis" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dimensionality-reduction.html"/>
<link rel="next" href="conclusion.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Index</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#table-of-contents"><i class="fa fa-check"></i>Table of Contents</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="univariate-analysis.html"><a href="univariate-analysis.html"><i class="fa fa-check"></i><b>2</b> Univariate Analysis</a></li>
<li class="chapter" data-level="3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>3</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="4" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>4</b> Cluster Analysis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#feasibility-of-the-cluster-analysis"><i class="fa fa-check"></i><b>4.1</b> Feasibility of the cluster analysis</a></li>
<li class="chapter" data-level="4.2" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hard-clustering"><i class="fa fa-check"></i><b>4.2</b> Hard clustering</a></li>
<li class="chapter" data-level="4.3" data-path="cluster-analysis.html"><a href="cluster-analysis.html#soft-clustering"><i class="fa fa-check"></i><b>4.3</b> Soft Clustering</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">College analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cluster-analysis" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Cluster Analysis<a href="cluster-analysis.html#cluster-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter focuses on <strong>clustering analysis</strong>, with the aim of identifying pattern in the data. The adopted approach is structured into multiple stages, each designed to evaluate, through different methods, the feasibility and robustness of clustering.</p>
<div id="feasibility-of-the-cluster-analysis" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Feasibility of the cluster analysis<a href="cluster-analysis.html#feasibility-of-the-cluster-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This first part investigates the <strong>feasibility of clustering</strong> by means of several exploratory techniques. Specifically, scatter plots of the variables colored according to the binary variable Private are compared with scatter plots generated from random values, in order to assess the presence of non-random patterns. The analysis is then extended to the first two principal components obtained via PCA. To further explore the correlation structure among observations, the <em>VAT (Visual Assessment of Tendency)</em> algorithm is applied, and the <em>Hopkins</em> statistic is computed to measure the clustering tendency of the dataset.</p>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-27-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-28-1.png" width="768" /></p>
</div>
</div>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-29-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-30-1.png" width="768" /></p>
</div>
</div>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-31-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-32-1.png" width="768" /></p>
</div>
</div>
<p>Both the <strong>VAT algorithm</strong> and the <strong>Hopkins statistic</strong> (0.1389425) indicate the presence of clusters in the dataset.</p>
</div>
<div id="hard-clustering" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Hard clustering<a href="cluster-analysis.html#hard-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This part is devoted to the application of hard clustering algorithms.</p>
<div id="hierarchical-clustering" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Hierarchical clustering<a href="cluster-analysis.html#hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before implementing hierarchical clustering, it is useful to compute the <strong>correlation between the cophenetic distances and the original distance matrix</strong> for each combination of distance and linkage method.</p>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Type
</th>
<th style="text-align:left;">
Correlations
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
euclidean_single
</td>
<td style="text-align:left;">
0.7038
</td>
</tr>
<tr>
<td style="text-align:left;">
euclidean_complete
</td>
<td style="text-align:left;">
0.6547
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
euclidean_average
</td>
<td style="text-align:left;font-weight: bold;">
0.7987
</td>
</tr>
<tr>
<td style="text-align:left;">
euclidean_ward.D2
</td>
<td style="text-align:left;">
0.4471
</td>
</tr>
<tr>
<td style="text-align:left;">
manhattan_single
</td>
<td style="text-align:left;">
0.6272
</td>
</tr>
<tr>
<td style="text-align:left;">
manhattan_complete
</td>
<td style="text-align:left;">
0.5267
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
manhattan_average
</td>
<td style="text-align:left;font-weight: bold;">
0.7605
</td>
</tr>
<tr>
<td style="text-align:left;">
manhattan_ward.D2
</td>
<td style="text-align:left;">
0.4633
</td>
</tr>
</tbody>
</table>
<p>The combinations with the highest cophenetic correlations are euclidean_average and manhattan_average. For both, a <strong>dendrogram</strong> is constructed, the <strong>optimal number of clusters</strong> (k) is identified, and the <strong>tree is pruned</strong> at this level.</p>
<div id="hierarchical-clustering-with-euclidean-distance-and-average-linked-method" class="section level4 unnumbered hasAnchor">
<h4>Hierarchical clustering with euclidean distance and average linked method<a href="cluster-analysis.html#hierarchical-clustering-with-euclidean-distance-and-average-linked-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-538-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-539-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-540-1.png" width="768" /></p>
</div>
</div>
<div id="nbclust-function" class="section level5 unnumbered hasAnchor">
<h5>NbClust function<a href="cluster-analysis.html#nbclust-function" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<pre><code>## [1] &quot;Frey index : No clustering structure in this data set&quot;</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-541-1.png" width="768" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-541-2.png" width="768" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 8 proposed 2 as the best number of clusters 
## * 4 proposed 3 as the best number of clusters 
## * 1 proposed 4 as the best number of clusters 
## * 7 proposed 5 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 1 proposed 9 as the best number of clusters 
## * 1 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
</div>
<div id="two-clusters" class="section level5 unnumbered hasAnchor">
<h5>Two clusters<a href="cluster-analysis.html#two-clusters" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="hscroll-plot" style="height:550px">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-542-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-543-1.png" width="768" /></p>
</div>
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1  776          0.68
## 2       2    1          0.00</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-544-1.png" width="768" /></p>
</div>
</div>
</div>
<div id="five-clusters" class="section level5 unnumbered hasAnchor">
<h5>Five clusters<a href="cluster-analysis.html#five-clusters" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="hscroll-plot" style="height:650px">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-545-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-546-1.png" width="768" /></p>
</div>
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1  766          0.48
## 2       2    8          0.51
## 3       3    1          0.00
## 4       4    1          0.00
## 5       5    1          0.00</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-547-1.png" width="768" /></p>
</div>
</div>
<p>Based on the results, the hierarchical clusterings with euclidean distance and the average linkage method, for <span class="math inline">\(k = 2\)</span> or <span class="math inline">\(k = 5\)</span>, <strong>do not yield meaningful results</strong>.</p>
</div>
</div>
<div id="hierarchical-clustering-with-manhattan-distance-and-average-linked-method" class="section level4 unnumbered hasAnchor">
<h4>Hierarchical clustering with manhattan distance and average linked method<a href="cluster-analysis.html#hierarchical-clustering-with-manhattan-distance-and-average-linked-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-548-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-549-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-550-1.png" width="768" /></p>
</div>
</div>
<div id="nbclust-function-1" class="section level5 unnumbered hasAnchor">
<h5>NbClust function<a href="cluster-analysis.html#nbclust-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="index_files/figure-html/unnamed-chunk-551-1.png" width="768" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-551-2.png" width="768" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 11 proposed 2 as the best number of clusters 
## * 1 proposed 3 as the best number of clusters 
## * 1 proposed 5 as the best number of clusters 
## * 2 proposed 6 as the best number of clusters 
## * 1 proposed 7 as the best number of clusters 
## * 6 proposed 8 as the best number of clusters 
## * 2 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
</div>
<div id="two-clusters-1" class="section level5 unnumbered hasAnchor">
<h5>Two clusters<a href="cluster-analysis.html#two-clusters-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="hscroll-plot" style="height:550px">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-552-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-553-1.png" width="768" /></p>
</div>
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1  776          0.53
## 2       2    1          0.00</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-554-1.png" width="768" /></p>
</div>
</div>
</div>
<div id="eight-clusters" class="section level5 unnumbered hasAnchor">
<h5>Eight clusters<a href="cluster-analysis.html#eight-clusters" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="hscroll-plot" style="height:750px">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-555-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-556-1.png" width="768" /></p>
</div>
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1  604          0.28
## 2       2   90          0.27
## 3       3   70          0.19
## 4       4    8          0.30
## 5       5    2          0.49
## 6       6    1          0.00
## 7       7    1          0.00
## 8       8    1          0.00</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-557-1.png" width="768" /></p>
</div>
</div>
<p>Based on the results, the hierarchical clusterings with manhattan distance and the average linkage method, for k = 2 or k = 8, <strong>do not yield meaningful results</strong>.</p>
</div>
</div>
</div>
<div id="partional-clustering" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Partional clustering<a href="cluster-analysis.html#partional-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let us apply algorithms of partitioning clustering, such as <strong>k-means</strong> and <strong>k-medoids</strong>. We also apply <strong>k-means++</strong>, which selects the initial centroids in a way that reduces the probability of choosing centroids that are too close to each other.</p>
<p>We observe that, for <span class="math inline">\(k = 2\)</span>, k-means++ successfully identifies the separation between private and public colleges, unlike the classic k-means. However, for <span class="math inline">\(k = 3\)</span>, the behavior is essentially the same.</p>
<div id="k-means" class="section level4 unnumbered hasAnchor">
<h4>K-means<a href="cluster-analysis.html#k-means" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-582-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-583-1.png" width="768" /></p>
</div>
</div>
<div id="nbclust-function-2" class="section level5 unnumbered hasAnchor">
<h5>NbClust function<a href="cluster-analysis.html#nbclust-function-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="index_files/figure-html/unnamed-chunk-584-1.png" width="768" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-584-2.png" width="768" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 4 proposed 2 as the best number of clusters 
## * 15 proposed 3 as the best number of clusters 
## * 1 proposed 7 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 1 proposed 9 as the best number of clusters 
## * 1 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  3 
##  
##  
## *******************************************************************</code></pre>
</div>
<div id="three-clusters" class="section level5 unnumbered hasAnchor">
<h5>Three clusters<a href="cluster-analysis.html#three-clusters" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="hscroll-plot" style="height:700px">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-586-1.png" width="768" /></p>
</div>
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1   93          0.18
## 2       2  246          0.23
## 3       3  438          0.26</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-587-1.png" width="768" /></p>
</div>
</div>
<p>Classic k-means with <span class="math inline">\(k = 3\)</span> performs better than hierarchical clustering, but it is still difficult to clearly distinguish three separate clusters. It should also be noted that the total variance explained by the first two principal components is relatively low.</p>
<p>The silhouette plot shows that the elements within each cluster do not achieve a high silhouette score.</p>
</div>
</div>
<div id="k-means-1" class="section level4 unnumbered hasAnchor">
<h4>K-means++<a href="cluster-analysis.html#k-means-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-588-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-589-1.png" width="768" /></p>
</div>
</div>
<div id="two-clusters-2" class="section level5 unnumbered hasAnchor">
<h5>Two clusters<a href="cluster-analysis.html#two-clusters-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="hscroll-plot" style="height:700px">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-591-1.png" width="768" /></p>
</div>
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1  676          0.36
## 2       2  101          0.20</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-592-1.png" width="768" /></p>
</div>
</div>
<pre><code>##                  true_labels
## pred_labelsKmeans  No Yes
##               No   88  13
##               Yes 124 552</code></pre>
<p>The k-means++, with some errors, is able to identify the private and the public college and the average silhouette width has a better score than others algorithms.</p>
</div>
</div>
<div id="k-medoids" class="section level4 unnumbered hasAnchor">
<h4>K-medoids<a href="cluster-analysis.html#k-medoids" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-594-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-595-1.png" width="768" /></p>
</div>
</div>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-596-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-597-1.png" width="768" /></p>
</div>
</div>
<div class="hscroll-plot" style="height:700px">
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1  388          0.27
## 2       2  237          0.23
## 3       3  152          0.10</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-598-1.png" width="768" /></p>
</div>
<div class="plot-container">
<pre><code>##   cluster size ave.sil.width
## 1       1  367          0.32
## 2       2  236          0.22
## 3       3  174          0.11</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-599-1.png" width="768" /></p>
</div>
</div>
<p>The K-medoids algorithm is more robust to outliers, and when using the Manhattan distance instead of the Euclidean distance, it yields better results in terms of the average silhouette width.</p>
</div>
</div>
<div id="density-based-clustering" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Density-based clustering<a href="cluster-analysis.html#density-based-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Density-based clustering offers an alternative approach to partitioning methods, as it does not require the number of clusters to be specified in advance. The underlying idea is that clusters correspond to high-density regions in the data space, separated by low-density areas. In this section, two main algorithms are applied: <strong>DBSCAN</strong> (Density-Based Spatial Clustering of Applications with Noise), which identifies groups of points with sufficient density and distinguishes noise points, and <strong>OPTICS</strong> (Ordering Points To Identify the Clustering Structure), which extends DBSCAN by enabling the detection of clusters with varying densities.</p>
<div id="dbscan" class="section level4 unnumbered hasAnchor">
<h4>DBSCAN<a href="cluster-analysis.html#dbscan" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Setting parameters for DBSCAN</strong> | From <a href="https://cran.r-project.org/web/packages/dbscan/refman/dbscan.html#dbscan">Documentation of dbscan package</a> function <code>dbscan</code></p>
<p>The parameters minPts and eps define the minimum density required in the area around core points which form the backbone of clusters. minPts is the number of points required in the neighborhood around the point defined by the parameter eps (i.e., the radius around the point). Both parameters depend on each other and changing one typically requires changing the other one as well. The parameters also depend on the size of the data set with larger datasets requiring a larger minPts or a smaller eps.</p>
<ul>
<li><p>⁠minPts:⁠ The original DBSCAN paper (Ester et al, 1996) suggests to start by setting <span class="math inline">\(minPts \geq d + 1\)</span>, the data dimensionality plus one or higher with a minimum of 3. Larger values are preferable since increasing the parameter suppresses more noise in the data by requiring more points to form clusters. Sander et al (1998) uses in the examples two times the data dimensionality. Note that setting
<span class="math inline">\(minPts \leq 2\)</span> is equivalent to hierarchical clustering with the single link metric and the dendrogram cut at height eps.</p></li>
<li><p>⁠eps:⁠ A suitable neighborhood size parameter eps given a fixed value for minPts can be found visually by inspecting the kNNdistplot() of the data using <span class="math inline">\(k=minPts−1\)</span> (minPts includes the point itself, while the k-nearest neighbors distance does not). The k-nearest neighbor distance plot sorts all data points by their k-nearest neighbor distance. A sudden increase of the kNN distance (a knee) indicates that the points to the right are most likely outliers. Choose eps for DBSCAN where the knee is.</p></li>
</ul>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-621-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-623-1.png" width="768" /></p>
</div>
</div>
<p>Here, the elbow method is somewhat difficult to interpret because there is no clear gap, and the resulting plot (on the right) is not satisfactory.</p>
</div>
<div id="optics" class="section level4 unnumbered hasAnchor">
<h4>Optics<a href="cluster-analysis.html#optics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This implementation of OPTICS implements the original algorithm as described by Ankerst et al (1999). OPTICS is an ordering algorithm with methods to extract a clustering from the ordering.</p>
<p>OPTICS linearly orders the data points such that points which are spatially closest become neighbors in the ordering. The closest analog to this ordering is dendrogram in single-link hierarchical clustering.</p>
<p><img src="index_files/figure-html/unnamed-chunk-625-1.png" width="768" /></p>
<p><strong>Extracting a clustering</strong> | From <a href="https://cran.r-project.org/web/packages/dbscan/refman/dbscan.html#optics">Documentation of dbscan package</a> function <code>optics</code></p>
<p>Several methods to extract a clustering from the order returned by OPTICS are implemented:</p>
<ul>
<li><p><code>extractDBSCAN()</code> extracts a clustering from an OPTICS ordering that is similar to what DBSCAN would produce with an eps set to eps_cl (see Ankerst et al, 1999). The only difference to a DBSCAN clustering is that OPTICS is not able to assign some border points and reports them instead as noise.</p></li>
<li><p><code>extractXi()</code> extract clusters hierarchically specified in Ankerst et al (1999) based on the steepness of the reachability plot. One interpretation of the xi parameter is that it classifies clusters by change in relative cluster density. The used algorithm was originally contributed by the ELKI framework and is explained in Schubert et al (2018), but contains a set of fixes.</p></li>
</ul>
</div>
<div id="extractdbscan-with-eps_cl1.61" class="section level4 unnumbered hasAnchor">
<h4>extractDBSCAN() with eps_cl=1.61<a href="cluster-analysis.html#extractdbscan-with-eps_cl1.61" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-627-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-628-1.png" width="768" /></p>
</div>
</div>
</div>
<div id="extractxi-with-xi0.01" class="section level4 unnumbered hasAnchor">
<h4>extractXi() with xi=0.01<a href="cluster-analysis.html#extractxi-with-xi0.01" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-630-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-631-1.png" width="768" /></p>
</div>
</div>
<p>Finally, <strong>both DBSCAN and OPTICS yield unsatisfactory results</strong>.</p>
</div>
</div>
<div id="cluster-validation" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Cluster validation<a href="cluster-analysis.html#cluster-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="external-clustering-validation" class="section level4 hasAnchor" number="4.2.4.1">
<h4><span class="header-section-number">4.2.4.1</span> External clustering validation<a href="cluster-analysis.html#external-clustering-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is possible apply only on the result of K-means++ (<span class="math inline">\(k=2\)</span>), because it is the unique clustering algorithem that identify the beahiour of the private and public college.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
Metric
</th>
<th style="text-align:center;">
Value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
Corrected Rand Index
</td>
<td style="text-align:center;">
0.3447
</td>
</tr>
<tr>
<td style="text-align:center;">
Meila’s VI Index
</td>
<td style="text-align:center;">
0.7293
</td>
</tr>
</tbody>
</table>
<p>Meila’s VI Index is not very high (its range is <span class="math inline">\((0, \infty)\)</span>), but the Corrected Rand Index is low.</p>
</div>
<div id="stability-measures" class="section level4 hasAnchor" number="4.2.4.2">
<h4><span class="header-section-number">4.2.4.2</span> Stability measures<a href="cluster-analysis.html#stability-measures" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
Method
</th>
<th style="text-align:center;">
APN
</th>
<th style="text-align:center;">
AD
</th>
<th style="text-align:center;">
ADM
</th>
<th style="text-align:center;">
FOM
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
HC - Euclidean - Avg - k=2
</td>
<td style="text-align:center;">
0.0003
</td>
<td style="text-align:center;">
5.3248
</td>
<td style="text-align:center;">
0.0075
</td>
<td style="text-align:center;">
0.9988
</td>
</tr>
<tr>
<td style="text-align:center;">
HC - Euclidean - Avg - k=5
</td>
<td style="text-align:center;">
0.0090
</td>
<td style="text-align:center;">
5.1870
</td>
<td style="text-align:center;">
0.1195
</td>
<td style="text-align:center;">
0.9812
</td>
</tr>
<tr>
<td style="text-align:center;">
HC - Manhattan - Avg - k=2
</td>
<td style="text-align:center;">
0.0100
</td>
<td style="text-align:center;">
17.0750
</td>
<td style="text-align:center;">
0.1067
</td>
<td style="text-align:center;">
0.9968
</td>
</tr>
<tr>
<td style="text-align:center;">
HC - Manhattan - Avg - k=8
</td>
<td style="text-align:center;">
0.0762
</td>
<td style="text-align:center;">
14.6278
</td>
<td style="text-align:center;">
0.7625
</td>
<td style="text-align:center;">
0.8614
</td>
</tr>
<tr>
<td style="text-align:center;">
KMeans - k=3
</td>
<td style="text-align:center;">
0.0446
</td>
<td style="text-align:center;">
4.3166
</td>
<td style="text-align:center;">
0.1900
</td>
<td style="text-align:center;">
0.8142
</td>
</tr>
<tr>
<td style="text-align:center;">
KMeans++ - k=2
</td>
<td style="text-align:center;">
0.2790
</td>
<td style="text-align:center;">
5.1583
</td>
<td style="text-align:center;">
1.4171
</td>
<td style="text-align:center;">
0.9792
</td>
</tr>
<tr>
<td style="text-align:center;">
KMedoids - Euclidean - k=3
</td>
<td style="text-align:center;">
0.1047
</td>
<td style="text-align:center;">
4.3950
</td>
<td style="text-align:center;">
0.4561
</td>
<td style="text-align:center;">
0.8357
</td>
</tr>
<tr>
<td style="text-align:center;">
KMedoids - Manhattan - k=3
</td>
<td style="text-align:center;">
0.1116
</td>
<td style="text-align:center;">
13.6128
</td>
<td style="text-align:center;">
0.4865
</td>
<td style="text-align:center;">
0.8384
</td>
</tr>
</tbody>
</table>
<pre><code>## Best APN: &quot;HC - Euclidean - Avg - k=2&quot;
## Best AD: &quot;KMeans - k=3&quot;
## Best ADM: &quot;HC - Euclidean - Avg - k=2&quot;
## Best FOM: &quot;KMeans - k=3&quot;</code></pre>
<p>From these stability measures, we can see that the most stable clustering algorithms are HC with Euclidean distance and average linkage (<span class="math inline">\(k=2\)</span>), and K-means (<span class="math inline">\(k=3\)</span>). However, the first solution yields only two clusters (with one of them has only one element). Therefore, K-means with <span class="math inline">\(k=3\)</span> is preferable.</p>
<p>Finally, we should choose between k-means with <span class="math inline">\(k=3\)</span> and k-means++ with <span class="math inline">\(k=2\)</span>.</p>
</div>
</div>
</div>
<div id="soft-clustering" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Soft Clustering<a href="cluster-analysis.html#soft-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section is devoted to the application of soft clustering approaches, such as <strong>fuzzy clustering</strong> and <strong>model-based clustering</strong>.</p>
<div id="fuzzy-clustering" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Fuzzy Clustering<a href="cluster-analysis.html#fuzzy-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here we compare the results of <strong>FKMEN</strong> with those of <strong>FKMedN</strong>. Other algorithms, such as <strong>GK-FKM</strong> and <strong>GKB-FK</strong>, were also tested, but they required too much computation time to provide results.</p>
<div id="fuzzy-k-means-with-entropy-regularization-and-noise-cluster" class="section level4 unnumbered hasAnchor">
<h4>Fuzzy k-Means with Entropy regularization and Noise cluster<a href="cluster-analysis.html#fuzzy-k-means-with-entropy-regularization-and-noise-cluster" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To select the optimal value of <span class="math inline">\(k\)</span>, we rely on fuzziness measures such as:</p>
<ul>
<li>Partition Coefficient (PC):</li>
</ul>
<pre><code>##    PC k=2    PC k=3    PC k=4    PC k=5    PC k=6 
## 0.7949485 0.8783218 0.8910608 0.8914083 0.8325891</code></pre>
<p>Best value at 5</p>
<ul>
<li>Modified Partition Coefficient (MPC)</li>
</ul>
<pre><code>##   MPC k=2   MPC k=3   MPC k=4   MPC k=5   MPC k=6 
## 0.5898970 0.8174826 0.8547477 0.8642603 0.7991070</code></pre>
<p>Best value at 5</p>
<ul>
<li>Partition Entropy (PE)</li>
</ul>
<pre><code>##     PE k=2     PE k=3     PE k=4     PE k=5     PE k=6 
## 0.07165039 0.08514369 0.10977413 0.13535170 0.23454366</code></pre>
<p>Best value at 2</p>
<p>and on compactness/separation measures such as:</p>
<ul>
<li>Xie and Beni (XB) index</li>
</ul>
<pre><code>##    XB k=2    XB k=3    XB k=4    XB k=5    XB k=6 
## 0.4991835 0.5150927 0.7636770 0.8387996 1.3164018</code></pre>
<p>Best value at 2</p>
<ul>
<li>Fuzzy Silhouette (FS) index</li>
</ul>
<pre><code>## SIL.F k=2 SIL.F k=3 SIL.F k=4 SIL.F k=5 SIL.F k=6 
## 0.3518378 0.3735748 0.3156925 0.2980328 0.2237030</code></pre>
<p>Best value at 3</p>
<p>According to these criteria, the optimal number of clusters is <span class="math inline">\(k=2\)</span>, since the Fuzzy Silhouette (FS) index reaches the second highest value at <span class="math inline">\(k=2\)</span>.</p>
<p>Therefore, we apply FKMEN with <span class="math inline">\(k=2\)</span>.</p>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-669-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-670-1.png" width="768" /></p>
</div>
</div>
<div class="datatables html-widget html-fill-item" id="htmlwidget-77e7346417e543420a49" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-77e7346417e543420a49">{"x":{"filter":"none","vertical":false,"data":[["Abilene Christian University","Adelphi University","Adrian College","Agnes Scott College","Alaska Pacific University","Albertson College","Albertus Magnus College","Albion College","Albright College","Alderson-Broaddus College","Alfred University","Allegheny College","Allentown Coll. of St. Francis de Sales","Alma College","Alverno College","American International College","Amherst College","Anderson University","Andrews University","Angelo State University","Antioch University","Appalachian State University","Aquinas College","Arizona State University Main campus","Arkansas College (Lyon College)","Arkansas Tech University","Assumption College","Auburn University-Main Campus","Augsburg College","Augustana College IL","Augustana College","Austin College","Averett College","Baker University","Baldwin-Wallace College","Barat College","Bard College","Barnard College","Barry University","Baylor University","Beaver College","Bellarmine College","Belmont Abbey College","Belmont University","Beloit College","Bemidji State University","Benedictine College","Bennington College","Bentley College","Berry College","Bethany College","Bethel College KS","Bethel College","Bethune Cookman College","Birmingham-Southern College","Blackburn College","Bloomsburg Univ. of Pennsylvania","Bluefield College","Bluffton College","Boston University","Bowdoin College","Bowling Green State University","Bradford College","Bradley University","Brandeis University","Brenau University","Brewton-Parker College","Briar Cliff College","Bridgewater College","Brigham Young University at Provo","Brown University","Bryn Mawr College","Bucknell University","Buena Vista College","Butler University","Cabrini College","Caldwell College","California Lutheran University","California Polytechnic-San Luis","California State University at Fresno","Calvin College","Campbell University","Campbellsville College","Canisius College","Capital University","Capitol College","Carleton College","Carnegie Mellon University","Carroll College","Carson-Newman College","Carthage College","Case Western Reserve University","Castleton State College","Catawba College","Catholic University of America","Cazenovia College","Cedar Crest College","Cedarville College","Centenary College","Centenary College of Louisiana"],[2,2,2,1,2,2,1,1,1,2,1,1,1,1,2,2,1,2,2,2,1,2,2,2,1,2,1,2,2,1,2,1,2,2,2,2,1,1,2,2,1,1,2,2,1,2,2,1,1,2,2,2,2,2,1,2,2,2,2,1,1,2,2,1,1,2,2,2,1,1,1,1,1,2,1,2,2,1,2,2,1,2,2,1,1,2,1,1,2,2,2,1,2,2,1,2,1,2,2,1],[0.9999972076224333,0.1110815790682443,0.9992632384923105,0.9999841640318249,0.9995103523882757,0.7586946650766174,0.9983476324045009,0.9999997692217544,0.999956052944899,0.9999859991708679,0.9999999107991944,0.9999999429404802,0.8423892817525962,0.9999379138101311,0.9999956258135342,0.9991637606397783,0.001361273639560007,0.9999984800415455,0.9999896913231004,0.9999754723189159,2.647796286703844e-11,0.05438612537267474,0.9995108916467746,2.220446049250313e-16,0.9942925320683491,0.9999991464829944,0.9999962108113253,0.0003614829125128395,0.999122559922041,0.9999333301709212,0.9403976735371965,0.9999776208493366,0.999999271594458,0.9999512682401075,0.5331378536737446,0.8923028594375282,0.9999947773702996,0.9992150872000317,0.9430695451286062,0.8634025477504063,0.9996117784064484,0.8155527044038297,0.9998607782873534,0.9998372265080105,0.9999395902119621,0.9999996944269667,0.9999381693148125,0.2933004190810163,0.9995634893263142,0.9576872526940349,0.999988306101633,0.9970786270224135,0.999999470191726,0.9999947636762108,0.9999881412454026,0.9751523253146261,0.9999919542448075,0.9999999605055505,0.9947624522358322,2.220446049250313e-16,1.683799797925943e-14,0.0004877806079749301,0.5933993924155946,2.220446049250313e-16,0.9999905064033907,0.9982261101874335,0.9999019183952502,0.9999954375719096,0.5700744126681091,2.220446049250313e-16,1.4028136268109e-05,0.992314235849878,0.99996924198327,0.5466682607355824,0.9999199678712226,0.9123307754275858,0.7183916184646243,0.6471761680703537,0.8425660300344864,0.9846410258077968,0.9484050283469992,0.9998376778781237,0.9999999204110751,0.8301099203868781,0.9765064071104116,0.8614863241517413,0.9359887140254229,0.9526567439556763,0.6558483858462649,0.9998755118241835,0.938349160828821,0.9865096035664596,0.9960723833917615,0.9990514737738988,0.9999963530370815,0.003766005733670209,0.9995843762693355,0.9999211717570395,0.999033115440949,0.8539486888732822]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Cluster<\/th>\n      <th>Membership degree<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0},{"name":" ","targets":0},{"name":"Cluster","targets":1},{"name":"Membership degree","targets":2}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>The resulting clusters have the following characteristics:</p>
<pre><code>## Clus 1 Clus 2 
##    334    443</code></pre>
<p>The centroids are given by:</p>
<div class="hscroll-plot" style="height: 300px;">
<div class="plot-container">
<table>
<colgroup>
<col width="4%" />
<col width="4%" />
<col width="4%" />
<col width="4%" />
<col width="6%" />
<col width="6%" />
<col width="7%" />
<col width="7%" />
<col width="5%" />
<col width="6%" />
<col width="4%" />
<col width="5%" />
<col width="4%" />
<col width="5%" />
<col width="6%" />
<col width="7%" />
<col width="4%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Apps</th>
<th align="right">Accept</th>
<th align="right">Enroll</th>
<th align="right">Top10perc</th>
<th align="right">Top25perc</th>
<th align="right">F.Undergrad</th>
<th align="right">P.Undergrad</th>
<th align="right">Outstate</th>
<th align="right">Room.Board</th>
<th align="right">Books</th>
<th align="right">Personal</th>
<th align="right">PhD</th>
<th align="right">Terminal</th>
<th align="right">S.F.Ratio</th>
<th align="right">perc.alumni</th>
<th align="right">Expend</th>
<th align="right">Grad.Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Clus 1</td>
<td align="right">-0.215</td>
<td align="right">-0.205</td>
<td align="right">-0.313</td>
<td align="right">0.472</td>
<td align="right">0.553</td>
<td align="right">-0.358</td>
<td align="right">-0.329</td>
<td align="right">0.798</td>
<td align="right">0.570</td>
<td align="right">-0.095</td>
<td align="right">-0.422</td>
<td align="right">0.538</td>
<td align="right">0.565</td>
<td align="right">-0.426</td>
<td align="right">0.671</td>
<td align="right">0.333</td>
<td align="right">0.624</td>
</tr>
<tr class="even">
<td align="left">Clus 2</td>
<td align="right">-0.326</td>
<td align="right">-0.310</td>
<td align="right">-0.265</td>
<td align="right">-0.565</td>
<td align="right">-0.578</td>
<td align="right">-0.237</td>
<td align="right">-0.097</td>
<td align="right">-0.592</td>
<td align="right">-0.489</td>
<td align="right">-0.173</td>
<td align="right">0.068</td>
<td align="right">-0.542</td>
<td align="right">-0.575</td>
<td align="right">0.295</td>
<td align="right">-0.421</td>
<td align="right">-0.491</td>
<td align="right">-0.464</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The confusion matrix is:</p>
<pre><code>##      
##         1   2
##   No   32 180
##   Yes 299 266</code></pre>
<p>where the Rand index has value 0.0490135.</p>
</div>
<div id="fk-med-algorithm-with-noise-cluster-fk-medn-algorithm" class="section level4 unnumbered hasAnchor">
<h4>FK-Med algorithm with Noise cluster (FK MedN algorithm)<a href="cluster-analysis.html#fk-med-algorithm-with-noise-cluster-fk-medn-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>By Fuzzy Silhouette (FS) index:</p>
<pre><code>##  SIL.F k=2  SIL.F k=3  SIL.F k=4  SIL.F k=5  SIL.F k=6 
## 0.31672170 0.25337145 0.22778222 0.18049682 0.04819403</code></pre>
<p>The best value is 2.</p>
<p>Therefore, we apply FKMedN with <span class="math inline">\(k=2\)</span>.</p>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-679-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-680-1.png" width="768" /></p>
</div>
</div>
<div class="datatables html-widget html-fill-item" id="htmlwidget-ff5cd684579f580a49ce" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-ff5cd684579f580a49ce">{"x":{"filter":"none","vertical":false,"data":[["Abilene Christian University","Adelphi University","Adrian College","Agnes Scott College","Alaska Pacific University","Albertson College","Albertus Magnus College","Albion College","Albright College","Alderson-Broaddus College","Alfred University","Allegheny College","Allentown Coll. of St. Francis de Sales","Alma College","Alverno College","American International College","Amherst College","Anderson University","Andrews University","Angelo State University","Antioch University","Appalachian State University","Aquinas College","Arizona State University Main campus","Arkansas College (Lyon College)","Arkansas Tech University","Assumption College","Auburn University-Main Campus","Augsburg College","Augustana College IL","Augustana College","Austin College","Averett College","Baker University","Baldwin-Wallace College","Barat College","Bard College","Barnard College","Barry University","Baylor University","Beaver College","Bellarmine College","Belmont Abbey College","Belmont University","Beloit College","Bemidji State University","Benedictine College","Bennington College","Bentley College","Berry College","Bethany College","Bethel College KS","Bethel College","Bethune Cookman College","Birmingham-Southern College","Blackburn College","Bloomsburg Univ. of Pennsylvania","Bluefield College","Bluffton College","Boston University","Bowdoin College","Bowling Green State University","Bradford College","Bradley University","Brandeis University","Brenau University","Brewton-Parker College","Briar Cliff College","Bridgewater College","Brigham Young University at Provo","Brown University","Bryn Mawr College","Bucknell University","Buena Vista College","Butler University","Cabrini College","Caldwell College","California Lutheran University","California Polytechnic-San Luis","California State University at Fresno","Calvin College","Campbell University","Campbellsville College","Canisius College","Capital University","Capitol College","Carleton College","Carnegie Mellon University","Carroll College","Carson-Newman College","Carthage College","Case Western Reserve University","Castleton State College","Catawba College","Catholic University of America","Cazenovia College","Cedar Crest College","Cedarville College","Centenary College","Centenary College of Louisiana"],[1,1,1,2,1,1,2,2,2,1,2,2,1,2,1,2,2,1,1,1,2,2,1,2,2,1,2,2,1,2,1,2,1,1,1,1,2,2,1,2,2,1,1,1,2,1,1,1,2,1,1,1,1,1,2,1,1,1,1,2,2,2,1,1,2,1,1,1,1,1,2,2,2,1,2,2,2,1,2,2,2,1,1,2,1,1,2,2,2,1,2,2,2,1,2,1,2,1,1,2],[0.6790239405520049,0.3322147116444492,0.8792647009377836,0.4857580656963238,0.4826669286287368,0.7717227272255833,0.8527154019432368,0.9314277981417072,0.8356798411394198,0.7411973320435997,0.8372915695310467,0.7269908415773062,0.704986655921185,0.7874782585557304,0.7684691695946601,0.5851277339506945,0.1847320035297306,0.9005936222798135,0.5937978635443141,0.4201179948954553,0.1326946085336974,0.3104566703529713,0.8932296182182369,0.04503944960531186,0.489363053820595,0.5420612443510104,0.8521711359546781,0.2357192906328454,0.6600717790930174,0.7994733999354374,0.9101102544371286,0.8459058907938375,0.55882440606087,0.8863609423862552,0.4490003312635614,0.6035774314800785,0.517475966017241,0.3595695837929343,0.4631091252472463,0.35072544438947,0.9251530105177235,0.5871009646292367,0.4746816522758457,0.9082953468185978,0.9037802389894664,0.567830768148716,0.4600494400339363,0.4109406208470054,0.7843157029660719,0.5299052734591226,0.8835031839765576,0.819315250362831,0.6602411816225827,0.6098937678272313,0.4888848320298286,0.4899213847950942,0.5640530997048347,0.6603668365590387,0.7421747905884666,0.05619581693432368,0.07185383767029273,0.2115276005345037,0.4286536241316368,0.04765232277714057,0.5694351912017872,0.512687898339258,0.4407290561152174,0.8834005275488538,0.6640888642073618,0.05262030772264178,0.1597129789566597,0.3361119197291771,0.502331520911585,0.8141622029427025,0.7783145408697005,0.447827553553759,0.7890876459957764,0.5569751141385725,0.2937053775511604,0.3407847754536746,0.7095399654524621,0.4109730362117082,0.8121778990742976,0.88659264006901,0.6248258762867159,0.4734437595250204,0.297375399430561,0.3383468252281203,0.680770109704785,0.8353898105464174,0.7563793452775421,0.3507398496911096,0.6005327538132835,0.6064214319544414,0.8032261762436009,0.2930692681744418,0.5799116513047006,0.7815758608603017,0.5355341584169848,0.5419380019388828]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Cluster<\/th>\n      <th>Membership degree<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0},{"name":" ","targets":0},{"name":"Cluster","targets":1},{"name":"Membership degree","targets":2}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>The resulting clusters have the following characteristics:</p>
<pre><code>## Clus 1 Clus 2 
##    378    399</code></pre>
<p>The centroids are given by:</p>
<div class="hscroll-plot" style="height: 300px;">
<div class="plot-container">
<table>
<colgroup>
<col width="4%" />
<col width="4%" />
<col width="4%" />
<col width="4%" />
<col width="6%" />
<col width="6%" />
<col width="7%" />
<col width="7%" />
<col width="5%" />
<col width="6%" />
<col width="4%" />
<col width="5%" />
<col width="4%" />
<col width="5%" />
<col width="6%" />
<col width="7%" />
<col width="4%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Apps</th>
<th align="right">Accept</th>
<th align="right">Enroll</th>
<th align="right">Top10perc</th>
<th align="right">Top25perc</th>
<th align="right">F.Undergrad</th>
<th align="right">P.Undergrad</th>
<th align="right">Outstate</th>
<th align="right">Room.Board</th>
<th align="right">Books</th>
<th align="right">Personal</th>
<th align="right">PhD</th>
<th align="right">Terminal</th>
<th align="right">S.F.Ratio</th>
<th align="right">perc.alumni</th>
<th align="right">Expend</th>
<th align="right">Grad.Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Clus 1</td>
<td align="right">-0.516</td>
<td align="right">-0.496</td>
<td align="right">-0.582</td>
<td align="right">-0.258</td>
<td align="right">0.364</td>
<td align="right">-0.575</td>
<td align="right">-0.528</td>
<td align="right">0.000</td>
<td align="right">-0.463</td>
<td align="right">-0.148</td>
<td align="right">0.162</td>
<td align="right">-0.592</td>
<td align="right">-0.523</td>
<td align="right">-0.578</td>
<td align="right">-0.221</td>
<td align="right">-0.329</td>
<td align="right">-0.085</td>
</tr>
<tr class="even">
<td align="left">Clus 2</td>
<td align="right">-0.200</td>
<td align="right">-0.093</td>
<td align="right">-0.369</td>
<td align="right">-0.032</td>
<td align="right">-0.091</td>
<td align="right">-0.462</td>
<td align="right">-0.405</td>
<td align="right">0.742</td>
<td align="right">0.062</td>
<td align="right">-0.602</td>
<td align="right">-0.208</td>
<td align="right">0.756</td>
<td align="right">0.632</td>
<td align="right">-0.275</td>
<td align="right">0.263</td>
<td align="right">-0.049</td>
<td align="right">0.381</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The confusion matrix is:</p>
<pre><code>##      
##         1   2
##   No  105 107
##   Yes 273 292</code></pre>
<p>where the Rand index has value -6.9482421^{-4}.</p>
<!-- ## Probabilistic assumptions -->
</div>
</div>
<div id="model-based-clustering" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Model Based Clustering<a href="cluster-analysis.html#model-based-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One disadvantage of traditional clustering methods, such as hierarchical and partitioning clustering algorithms, is that they are largely heuristic and not based on formal models.</p>
<p>An alternative is model-based clustering that use <strong>probabilistic assumptions</strong>.</p>
<p><img src="index_files/figure-html/unnamed-chunk-715-1.png" width="768" /></p>
<pre><code>## Best BIC values:
##              VVE,9       VVE,8       VVE,7
## BIC      -18547.17 -18671.4418 -18925.0659
## BIC diff      0.00   -124.2717   -377.8958</code></pre>
<p>Using the <strong>Mclust</strong> function, the best model selected is <strong>VVE</strong>, which corresponds to clusters with variable volume and shape but equal orientation, with <span class="math inline">\(k=9\)</span>.</p>
<pre><code>## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust VVE (ellipsoidal, equal orientation) model with 9 components: 
## 
##  log-likelihood   n  df       BIC       ICL
##       -7776.111 777 450 -18547.17 -18669.83
## 
## Clustering table:
##   1   2   3   4   5   6   7   8   9 
##  52 101  90 125 112  94  46  54 103</code></pre>
<p>However, from a graphical point of view, the result is very difficult to interpret.</p>
<div class="hscroll-plot">
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-718-1.png" width="768" /></p>
</div>
<div class="plot-container">
<p><img src="index_files/figure-html/unnamed-chunk-719-1.png" width="768" /></p>
</div>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dimensionality-reduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
